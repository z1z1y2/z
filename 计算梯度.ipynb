{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3fa487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 的梯度:  tensor(0.0089)\n",
      "x2 的梯度:  tensor(0.0089)\n",
      "v1 的梯度:  None\n",
      "a 的梯度:  None\n",
      "b 的梯度:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_3592/2352120609.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"v1 的梯度: \", v1.grad)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_3592/2352120609.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"a 的梯度: \", a.grad)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_3592/2352120609.py:25: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"b 的梯度: \", b.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# 计算图中的节点\n",
    "v1 = x1**2 + 2*x2 + 1\n",
    "a = sigmoid(v1)\n",
    "b = 3*a + a + 1\n",
    "y = sigmoid(b)\n",
    "\n",
    "# 计算梯度\n",
    "y.backward(torch.tensor(1.0))\n",
    "\n",
    "# 打印梯度\n",
    "print(\"x1 的梯度: \", x1.grad)\n",
    "print(\"x2 的梯度: \", x2.grad)\n",
    "print(\"v1 的梯度: \", v1.grad)\n",
    "print(\"a 的梯度: \", a.grad)\n",
    "print(\"b 的梯度: \", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def11d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86424c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9eafe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点v0对x1导数:  tensor(1.)\n",
      "节点v0对x2导数:  tensor(0.)\n",
      "节点v1对x1导数:  tensor(0.)\n",
      "节点v1对x2导数:  tensor(1.)\n",
      "节点v2对x1导数:  tensor(2.)\n",
      "节点v2对x2导数:  tensor(0.)\n",
      "节点z对x1导数:  tensor(2.)\n",
      "节点z对x2导数:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def x_square(x):\n",
    "    return x**2\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "v0 = x1\n",
    "v1 = x2\n",
    "v2 = x_square(v0)\n",
    "z = v2+v1\n",
    "\n",
    "node_dict = {\"v0\": v0, \"v1\": v1, \"v2\": v2, \"z\": z}\n",
    "var_dict = {\"x1\": x1, \"x2\": x2}\n",
    "\n",
    "for node_name in node_dict:\n",
    "    for var_name in var_dict:\n",
    "        node = node_dict[node_name]\n",
    "        var = var_dict[var_name]\n",
    "        \n",
    "        if var.grad is not None:\n",
    "            var.grad.zero_()\n",
    "        else:\n",
    "            var.grad = torch.tensor(0.0)\n",
    "        node.backward(retain_graph=True)\n",
    "        print(f\"节点{node_name}对{var_name}导数: \", var.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef4114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1adbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e50139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbad086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6df212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6116da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc1f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12056d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6a78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659afa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a725ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125f706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d3611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
